mpi_msg_ping example application
================================

1) add to /etc/hosts localhostslurm1 and localhostslurm2 for ip 127.0.0.1
These are configured by default in the /opt/rocm_sdk_xyz/etc/slurm/slurm.conf
For example:
    127.0.0.1	localhost localhostslurm1 localhostslurm2

2) launch slurm controller in linux terminal
# source /opt/rocm_sdk_612/bin/env_rocm.sh
# /opt/rocm_sdk_612/bin/slurmctld -D

3) launch slurm worknode rocmsdk1 in linux terminal
# source /opt/rocm_sdk_612/bin/env_rocm.sh
# /opt/rocm_sdk_612/bin/slurmd -D -Nrocmsdk1

4) launch slurm worknode rocmsdk2 in linux terminal
# source /opt/rocm_sdk_612/bin/env_rocm.sh
# /opt/rocm_sdk_612/bin/slurmd -D -Nrocmsdk2

5) build the mpi_msg_ping example application
# source /opt/rocm_sdk_612/bin/env_rocm.sh
# cd /opt/rocm_sdk_612/docs/tutorial/openmpi/mpi_msg_ping
# make

6) launch 4 slurm job instances of example application
srun -l -n4 -N2 ./mpi_msg_ping

7) Expected outcome is that all 4 job instances will send and receive ping
   messages. Example output below:
-------------------------------------
srun -l -n4 -N2 ./mpi_msg_ping
pidfile: /opt/rocm_sdk_612/var/run/slurm/slurmctld.pid
0: cpu-bind=MASK - rocmsdk1, task  0  0 [15715]: mask 0x1 set
0: cpu-bind=MASK - rocmsdk1, task  1  1 [15717]: mask 0x4 set
0: cpu-bind=MASK - rocmsdk1, task  2  2 [15718]: mask 0x10 set
3: cpu-bind=MASK - rocmsdk2, task  3  0 [15716]: mask 0x1 set
1: rank 1 received value ping_0 from mpi node 0
1: rank 1 received value ping_2 from mpi node 2
1: rank 1 received value ping_3 from mpi node 3
2: rank 2 received value ping_0 from mpi node 0
2: rank 2 received value ping_1 from mpi node 1
2: rank 2 received value ping_3 from mpi node 3
0: rank 0 received value ping_1 from mpi node 1
0: rank 0 received value ping_2 from mpi node 2
0: rank 0 received value ping_3 from mpi node 3
3: rank 3 received value ping_0 from mpi node 0
3: rank 3 received value ping_1 from mpi node 1
3: rank 3 received value ping_2 from mpi node 2
---------------------------------------------
